{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import joblib\n",
    "import mlflow\n",
    "from meteocalc import feels_like, heat_index, wind_chill, Temp\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "CURRENT_EXPERIMENT_NAME = 'catboost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    df_out = df\n",
    "    for key, value in kwargs.items():\n",
    "        if type(value) is list:\n",
    "            df_out = df_out[df_out[key].isin(value)]\n",
    "        else:\n",
    "            df_out = df_out[df_out[key] == value]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def missing_rate(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isnull().sum() / len(df)\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df: pd.DataFrame, verbose: bool = True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / (1024 ** 2)    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem)\n",
    "        )\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "rmse_score = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "def add_key_prefix(d: Dict, prefix = 'best_') -> Dict:\n",
    "    return {prefix + key: value for key, value in d.items()}\n",
    "\n",
    "\n",
    "def df_from_cv_results(d: Dict):\n",
    "    df = pd.DataFrame(d)\n",
    "    score_columns = ['mean_test_score', 'mean_train_score']\n",
    "    param_columns = [c for c in df.columns if c.startswith('param_')]\n",
    "    return pd.concat([\n",
    "        -df.loc[:, score_columns],\n",
    "        df.loc[:, param_columns],\n",
    "    ], axis=1).sort_values(by='mean_test_score')\n",
    "\n",
    "\n",
    "def sample(*args, frac: float = 0.01) -> np.ndarray:\n",
    "    n_rows = args[0].shape[0]\n",
    "    random_index = np.random.choice(n_rows, int(n_rows * frac), replace=False)\n",
    "    gen = (\n",
    "        a[random_index] for a in args\n",
    "    )\n",
    "    if len(args) == 1:\n",
    "        return next(gen)\n",
    "    else:\n",
    "        return gen\n",
    "\n",
    "    \n",
    "class BaseTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x: pd.DataFrame, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColumnTransformer(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, defs: Dict[str, BaseTransformer]):\n",
    "        self.defs = defs\n",
    "    \n",
    "    def fit(self, x: pd.DataFrame, y: np.ndarray = None):\n",
    "        for col, transformer in self.defs.items():\n",
    "            transformer.fit(x[col], y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        for col, transformer in self.defs.items():\n",
    "            xp[col] = transformer.transform(x[col])\n",
    "        return xp\n",
    "    \n",
    "    def fit_transform(self, x: pd.DataFrame, y: np.ndarray = None) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        for col, transformer in self.defs.items():\n",
    "            if hasattr(transformer, 'fit_transform'):\n",
    "                xp[col] = transformer.fit_transform(x[col], y)\n",
    "            else:\n",
    "                xp[col] = transformer.fit(x[col], y).transform(x[col])\n",
    "        return xp\n",
    "\n",
    "\n",
    "class WrappedLabelEncoder(BaseTransformer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.le = LabelEncoder()\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        self.le.fit(x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return self.le.transform(x)\n",
    "\n",
    "\n",
    "def wind_chill_safely(t, w):\n",
    "    try:\n",
    "        return wind_chill(t, w)\n",
    "    except ValueError:\n",
    "        return Temp(10, unit='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = pd.read_csv('data/rows_to_drop.csv')\n",
    "train = pd.read_csv('data/train.csv', parse_dates=['timestamp']).pipe(reduce_mem_usage).drop(index=rows_to_drop.iloc[:, 0])\n",
    "del rows_to_drop\n",
    "building_metadata = pd.read_csv('data/building_metadata.csv')\n",
    "weather_train = pd.read_csv('data/weather_train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "test = pd.read_csv('data/test.csv', parse_dates=['timestamp']).pipe(reduce_mem_usage)\n",
    "weather_test = pd.read_csv('data/weather_test.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherImputer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, w: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # add missing datetime\n",
    "        dt_min, dt_max = w['timestamp'].min(), w['timestamp'].max()\n",
    "        empty_df = pd.DataFrame({'timestamp': pd.date_range(start=dt_min, end=dt_max, freq='H')})\n",
    "        w_out = pd.concat([\n",
    "            ws.merge(\n",
    "                empty_df, on='timestamp', how='outer'\n",
    "            ).sort_values(\n",
    "                by='timestamp'\n",
    "            ).assign(\n",
    "                site_id=site_id\n",
    "            ) for site_id, ws in w.groupby('site_id')\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        w_out['month'] = w_out['timestamp'].dt.month\n",
    "        w_out['time_period'] = pd.cut(\n",
    "            w_out['timestamp'].dt.hour,\n",
    "            bins=[0, 3, 6, 9, 12, 15, 18, 21, 25],\n",
    "            right=False, labels=False,\n",
    "        )\n",
    "        w_out = w_out.set_index(['site_id', 'month', 'time_period'])\n",
    "        w_updater = w_out.groupby(['site_id', 'month', 'time_period']).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        w_out.update(w_updater, overwrite=False)  # destroying method\n",
    "        w_out = w_out.reset_index().drop(columns=['month', 'time_period'])\n",
    "\n",
    "        # float -> uint\n",
    "        w_out['site_id'] = w_out['site_id'].astype(np.uint8)\n",
    "\n",
    "        return w_out\n",
    "\n",
    "\n",
    "class WeatherEngineerer(BaseTransformer):\n",
    "    \n",
    "    @staticmethod\n",
    "    def shift_by(wdf: pd.DataFrame, n: int, cols: List[str]) -> pd.DataFrame:\n",
    "        method = 'bfill' if n > 0 else 'ffill'\n",
    "        return pd.concat([\n",
    "            ws.loc[:, cols].shift(n).fillna(method=method) for _, ws in wdf.groupby('site_id')\n",
    "        ], axis=0)\n",
    "    \n",
    "    def weighted_average(self, w: pd.DataFrame, cols: List[str], hours: int) -> pd.DataFrame:\n",
    "        ahours = abs(hours)\n",
    "        sign = int(hours / ahours)\n",
    "        w_weighted_average = sum(\n",
    "            [self.shift_by(w, (i+1)*sign, cols) * (ahours-i) for i in range(ahours)]\n",
    "        ) / (np.arange(ahours) + 1).sum()\n",
    "\n",
    "        w_weighted_average.columns = ['{0}_wa{1}'.format(c, hours) for c in cols]\n",
    "\n",
    "        return pd.concat([w, w_weighted_average], axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dwdt(df: pd.DataFrame, base_cols: List[str], suffixes: List[str]) -> pd.DataFrame:\n",
    "        df_out = df.copy()\n",
    "        for base_col in base_cols:\n",
    "            for suffix in suffixes:\n",
    "                df_out[base_col + '_dt' + suffix] = df_out[base_col + suffix] - df_out[base_col]\n",
    "        return df_out\n",
    "    \n",
    "    @staticmethod\n",
    "    def lag(w: pd.DataFrame, cols: List[str], window: int = 3):\n",
    "        \n",
    "        group_df = w.groupby('site_id')\n",
    "        rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "        lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "        lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "        lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "        lag_std = rolled.std().fillna(method='bfill').reset_index().astype(np.float16)\n",
    "        \n",
    "        w_out = w.copy()\n",
    "        for col in cols:\n",
    "            w_out['{0}_mean_{1}'.format(col, window)] = lag_mean[col]\n",
    "            w_out['{0}_max_{1}'.format(col, window)] = lag_max[col]\n",
    "            w_out['{0}_min_{1}'.format(col, window)] = lag_min[col]\n",
    "            w_out['{0}_std_{1}'.format(col, window)] = lag_std[col]\n",
    "        return w_out\n",
    "\n",
    "    @staticmethod\n",
    "    def meteocalc(w_in: pd.DataFrame, suffix: str = '') -> pd.DataFrame:\n",
    "\n",
    "        w = w_in.assign(**{\n",
    "            'relative_humidity' + suffix: 100 * (\n",
    "                np.exp(\n",
    "                    (17.625 * w_in['dew_temperature' + suffix]) / (243.04 + w_in['dew_temperature' + suffix])\n",
    "                ) / np.exp(\n",
    "                    (17.625 * w_in['air_temperature' + suffix]) / (243.04 + w_in['air_temperature' + suffix])\n",
    "                )\n",
    "            )\n",
    "        })\n",
    "        return w.assign(**{\n",
    "            'feels_like' + suffix: w.apply(lambda row: feels_like(\n",
    "                Temp(row['air_temperature' + suffix], unit='C'),\n",
    "                row['relative_humidity' + suffix],\n",
    "                row['wind_speed' + suffix]\n",
    "            ).c, axis=1)\n",
    "        }).assign(**{\n",
    "            'heat_index' + suffix: w.apply(lambda row: heat_index(\n",
    "                Temp(row['air_temperature' + suffix], unit='C'),\n",
    "                row['relative_humidity' + suffix]\n",
    "            ).c, axis=1)\n",
    "        }).assign(**{\n",
    "            'wind_chill' + suffix: w.apply(lambda row: wind_chill_safely(\n",
    "                Temp(row['air_temperature' + suffix], unit='C'),\n",
    "                row['wind_speed' + suffix]\n",
    "            ).c, axis=1)\n",
    "        })\n",
    "    \n",
    "    def transform(self, w_in: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        w = w_in.pipe(self.meteocalc)\n",
    "        \n",
    "        cols = ['air_temperature', 'wind_speed', 'feels_like', 'heat_index', 'wind_chill']\n",
    "\n",
    "        w = w.pipe(\n",
    "            self.weighted_average, hours=1, cols=cols\n",
    "        ).pipe(\n",
    "            self.weighted_average, hours=-1, cols=cols\n",
    "        ).pipe(\n",
    "            self.weighted_average, hours=5, cols=cols\n",
    "        ).pipe(\n",
    "            self.weighted_average, hours=-5, cols=cols\n",
    "        ).pipe(\n",
    "            self.dwdt, base_cols=cols, suffixes=['_wa1', '_wa-1', '_wa5', '_wa-5']\n",
    "        ).pipe(\n",
    "            self.lag, window=3, cols=cols\n",
    "        ).pipe(\n",
    "            self.lag, window=72, cols=cols\n",
    "        )\n",
    "        \n",
    "        return w\n",
    "\n",
    "\n",
    "class WindDirectionEncoder(BaseTransformer):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _from_degree(degree: int) -> int:\n",
    "        val = int((degree / 22.5) + 0.5)\n",
    "        arr = [i for i in range(0,16)]\n",
    "        return arr[(val % 16)]\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return x.apply(self._from_degree)\n",
    "\n",
    "\n",
    "class WindSpeedEncoder(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return pd.cut(\n",
    "            x,\n",
    "            bins=[0, 0.3, 1.6, 3.4, 5.5, 8, 10.8, 13.9, 17.2, 20.8, 24.5, 28.5, 33, 1000],\n",
    "            right=False, labels=False,\n",
    "        )\n",
    "\n",
    "    \n",
    "weather_pipeline = Pipeline(steps=[\n",
    "    ('impute_missing_value', WeatherImputer()),\n",
    "    ('feature_engineering', WeatherEngineerer()),\n",
    "    ('label_encode', ColumnTransformer({\n",
    "        'wind_direction': WindDirectionEncoder(),\n",
    "        'wind_speed': WindSpeedEncoder(),\n",
    "        'wind_speed_wa1': WindSpeedEncoder(),\n",
    "        'wind_speed_wa-1': WindSpeedEncoder(),\n",
    "        'wind_speed_wa5': WindSpeedEncoder(),\n",
    "        'wind_speed_wa-5': WindSpeedEncoder(),\n",
    "        'wind_speed_mean_3': WindSpeedEncoder(),\n",
    "        'wind_speed_mean_72': WindSpeedEncoder(),\n",
    "        'wind_speed_max_3': WindSpeedEncoder(),\n",
    "        'wind_speed_max_72': WindSpeedEncoder(),\n",
    "        'wind_speed_min_3': WindSpeedEncoder(),\n",
    "        'wind_speed_min_72': WindSpeedEncoder(),\n",
    "    }))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Metadata Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingMetadataEngineerer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, bm_in: pd.DataFrame) -> pd.DataFrame:\n",
    "        bm = bm_in.copy()\n",
    "        bm['log_square_feet'] = np.log1p(bm['square_feet'])\n",
    "        bm['square_feet_per_floor'] = bm['square_feet'] / bm['floor_count']\n",
    "        bm['log_square_feet_per_floor'] = bm['log_square_feet'] / bm['floor_count']\n",
    "        bm['building_age'] = 2019 - bm['year_built']\n",
    "        bm['square_feet_per_age'] = bm['square_feet'] / bm['building_age']\n",
    "        bm['log_square_feet_per_age'] = bm['log_square_feet'] / bm['building_age']\n",
    "        return bm\n",
    "\n",
    "\n",
    "class BuildingMetadataImputer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, bm: pd.DataFrame) -> pd.DataFrame:\n",
    "        return bm.fillna(-999)\n",
    "\n",
    "\n",
    "building_metadata_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', BuildingMetadataEngineerer()),\n",
    "    ('impute_missing_value', BuildingMetadataImputer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingMetaJoiner(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, bm: pd.DataFrame = None):\n",
    "        self.bm = bm\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.bm is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x.merge(\n",
    "                self.bm,\n",
    "                on='building_id',\n",
    "                how='left',\n",
    "            )\n",
    "\n",
    "    \n",
    "class WeatherJoiner(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, w: pd.DataFrame = None):\n",
    "        self.w = w\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.w is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x.merge(\n",
    "                self.w,\n",
    "                on=['site_id', 'timestamp'],\n",
    "                how='left',\n",
    "            )\n",
    "\n",
    "\n",
    "class DatetimeFeatureEngineerer(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, col: str = 'timestamp'):\n",
    "        self.col = col\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        ts = x[self.col]\n",
    "        xp['month'] = ts.dt.month.astype(np.int8)\n",
    "        xp['week'] = ts.dt.week.astype(np.int8)\n",
    "        xp['day_of_week'] = ts.dt.weekday.astype(np.int8)\n",
    "        xp['time_period'] = pd.cut(\n",
    "            ts.dt.hour,\n",
    "            bins=[0, 3, 6, 9, 12, 15, 18, 21, 25],\n",
    "            right=False, labels=False,\n",
    "        )\n",
    "        \n",
    "        holidays = [\n",
    "            '2016-01-01', '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04',\n",
    "            '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24', '2016-12-26',\n",
    "            '2017-01-01', '2017-01-16', '2017-02-20', '2017-05-29', '2017-07-04',\n",
    "            '2017-09-04', '2017-10-09', '2017-11-10', '2017-11-23', '2017-12-25',\n",
    "            '2018-01-01', '2018-01-15', '2018-02-19', '2018-05-28', '2018-07-04',\n",
    "            '2018-09-03', '2018-10-08', '2018-11-12', '2018-11-22', '2018-12-25',\n",
    "            '2019-01-01'\n",
    "        ]\n",
    "        xp['is_holiday'] = (ts.dt.date.astype('str').isin(holidays)).astype(np.int8)\n",
    "        return xp\n",
    "\n",
    "\n",
    "class TargetEncoder(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, cv: int = 5, smoothing: int = 1):\n",
    "        self.agg = None\n",
    "        self.cv = cv\n",
    "        self.smoothing = 1\n",
    "    \n",
    "    def transform(self, x: pd.Series):        \n",
    "        if self.agg is None:\n",
    "            raise ValueError('you shold fit() before predict()')\n",
    "        encoded = pd.merge(x, self.agg, left_on=x.name, right_index=True, how='left')\n",
    "        encoded = encoded.fillna(encoded.mean())\n",
    "        xp = encoded['y']\n",
    "        xp.name = x.name\n",
    "        return xp\n",
    "    \n",
    "    def fit_transform(self, x: pd.Series, y: np.ndarray = None) -> pd.Series:\n",
    "        df = pd.DataFrame({'x': x, 'y': y})\n",
    "        self.agg = df.groupby('x').mean()\n",
    "        fold = KFold(n_splits=self.cv, shuffle=True)\n",
    "        xp = x.copy()\n",
    "        for idx_train, idx_test in fold.split(x):\n",
    "            df_train = df.loc[idx_train, :]\n",
    "            df_test = df.loc[idx_test, :]\n",
    "            agg_train = df_train.groupby('x').mean()\n",
    "            encoded = pd.merge(df_test, agg_train, left_on='x', right_index=True, how='left', suffixes=('', '_mean'))['y_mean']\n",
    "            encoded = encoded.fillna(encoded.mean())\n",
    "            xp[encoded.index] = encoded\n",
    "        return xp\n",
    "\n",
    "\n",
    "class ColumnDropper(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, cols: List[str]):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame, y = None) -> pd.DataFrame:\n",
    "        return x.drop(columns=self.cols)\n",
    "\n",
    "\n",
    "class ArrayTransformer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame, y = None) -> np.ndarray:\n",
    "        return x.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_metadata_pipeline_out = building_metadata_pipeline.fit_transform(\n",
    "    building_metadata\n",
    ")\n",
    "weather_pipeline_out = weather_pipeline.fit_transform(\n",
    "    pd.concat([weather_train, weather_test], axis=0, ignore_index=True)\n",
    ")\n",
    "del building_metadata\n",
    "del weather_test\n",
    "del weather_train\n",
    "\n",
    "\n",
    "def pipeline_factory() -> Pipeline:\n",
    "    return Pipeline(steps=[\n",
    "\n",
    "        # join\n",
    "        ('join_building_meta', BuildingMetaJoiner(\n",
    "            building_metadata_pipeline_out\n",
    "        )),\n",
    "        ('join_weather', WeatherJoiner(\n",
    "            weather_pipeline_out\n",
    "        )),\n",
    "\n",
    "        # feature engineering\n",
    "        ('feature_engineering_from_datetime', DatetimeFeatureEngineerer()),\n",
    "#         ('target_encode', ColumnTransformer({\n",
    "#             'primary_use': TargetEncoder(),\n",
    "#             'meter': TargetEncoder(),\n",
    "#             'cloud_coverage': TargetEncoder(),\n",
    "#             'time_period': TargetEncoder(),\n",
    "#             'wind_direction': TargetEncoder(),\n",
    "#             'wind_speed': TargetEncoder(),\n",
    "#             'wind_speed_wa1': TargetEncoder(),\n",
    "#             'wind_speed_wa-1': TargetEncoder(),\n",
    "#             'wind_speed_wa5': TargetEncoder(),\n",
    "#             'wind_speed_wa-5': TargetEncoder(),\n",
    "#         })),\n",
    "\n",
    "        # drop columns\n",
    "        ('drop_columns', ColumnDropper([\n",
    "            'building_id', 'timestamp', 'site_id', 'precip_depth_1_hr',\n",
    "        ])),\n",
    "\n",
    "        # pd.DataFrame -> np.ndarray\n",
    "#         ('df_to_array', ArrayTransformer()),\n",
    "\n",
    "        # regressor\n",
    "#         ('regressor', RandomForestRegressor()),\n",
    "#         ('regressor', CatBoostRegressor()),\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pipeline_factory().fit_transform(\n",
    "    train.sample(frac=0.001).drop(columns='meter_reading')\n",
    ")\n",
    "tmp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'meter', 'primary_use', 'wind_direction', 'wind_speed',\n",
    "    'wind_speed_wa1', 'wind_speed_wa-1', 'wind_speed_wa5', 'wind_speed_wa-5',\n",
    "    'wind_speed_mean_3', 'wind_speed_max_3', 'wind_speed_min_3',\n",
    "    'wind_speed_mean_72', 'wind_speed_max_72', 'wind_speed_min_72',\n",
    "    'day_of_week', 'time_period', 'is_holiday',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_catboost(p: Pipeline,\n",
    "                df: pd.DataFrame,\n",
    "                n_splits: int = 7,\n",
    "                break_first: bool = False,\n",
    "                validate_every_iteration: bool = True,\n",
    "                **params):\n",
    "    \n",
    "    x = p.fit_transform(df.drop(columns='meter_reading'))\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        fold = KFold(n_splits=n_splits, shuffle=True)\n",
    "        scores = []\n",
    "\n",
    "        # switching train <-> val \n",
    "        for i_split, (i_val, i_train) in tqdm(enumerate(fold.split(x)), total=n_splits):\n",
    "            \n",
    "            i_val = np.random.choice(i_val, size=len(i_train))  # reduce val data size\n",
    "            \n",
    "            x_train, x_val = x.loc[i_train, :], x.loc[i_val, :]\n",
    "            y_train, y_val = y[i_train], y[i_val]\n",
    "            \n",
    "            model = CatBoostRegressor(**params)\n",
    "            fit_params = dict(cat_features=cat_features, logging_level='Verbose')\n",
    "            \n",
    "            if validate_every_iteration:\n",
    "                fit_params.update(dict(eval_set=(x_val, y_val)))\n",
    "            \n",
    "            model.fit(x_train, y_train, **fit_params)\n",
    "            rmse_train = model.best_score_['learn']['RMSE']\n",
    "            rmse_train_list = model.evals_result_['learn']['RMSE']\n",
    "                        \n",
    "            if validate_every_iteration:\n",
    "                rmse_val = model.best_score_['validation']['RMSE']\n",
    "                rmse_val_list = model.evals_result_['validation']['RMSE']\n",
    "            else:\n",
    "                rmse_val = rmse(y_val, model.predict(x_val))\n",
    "                rmse_val_list = np.zeros(len(rmse_train_list))  # dummy value\n",
    "                \n",
    "            scores.append(dict(\n",
    "                RMSE_train=rmse_train,\n",
    "                RMSE_val=rmse_val,\n",
    "            ))\n",
    "            mlflow.log_metrics(pd.DataFrame(scores).mean().to_dict())\n",
    "            csv_filename = 'out/eval_results{0}.csv'.format(i_split)\n",
    "            model_filename = 'out/model{0}.joblib'.format(i_split)\n",
    "            \n",
    "            if i_split == 0:\n",
    "                \n",
    "                pd.DataFrame({\n",
    "                    'RMSE_train': rmse_train_list,\n",
    "                    'RMSE_val': rmse_val_list\n",
    "                }).to_csv(csv_filename, index=False)\n",
    "                mlflow.log_artifact(csv_filename)\n",
    "                \n",
    "            joblib.dump(model, model_filename)\n",
    "            mlflow.log_artifact(model_filename)\n",
    "            if break_first:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_catboost(\n",
    "    pipeline_factory(),\n",
    "    train,\n",
    "    n_estimators=2,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    early_stopping_rounds=10,\n",
    "    validate_every_iteration=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_catboost(run_id: str = None, n_models: int = 7):\n",
    "    if run_id is None:\n",
    "        model_paths = ['out/model{0}.joblib'.format(i) for i in range(n_models)]\n",
    "    else:\n",
    "        c = mlflow.tracking.MlflowClient()\n",
    "        model_paths = [c.download_artifacts(run_id, 'model{0}.joblib'.format(i)) for i in range(n_models)]\n",
    "\n",
    "    return [joblib.load(p) for p in model_paths]\n",
    "\n",
    "\n",
    "def predict_catboost(df: pd.DataFrame, p: Pipeline, models, n_iter: int = 10) -> pd.DataFrame:\n",
    "    n_samples = len(df)\n",
    "    batch_size = n_samples // (n_iter - 1)\n",
    "    y = []\n",
    "    for i_iter in tqdm(range(n_iter), total=n_iter):\n",
    "        pos = i_iter * batch_size\n",
    "        x_batch = df.iloc[pos:pos + batch_size, 1:]\n",
    "        y_batch = np.expm1(\n",
    "            np.mean([\n",
    "                m.predict(p.transform(x_batch)) for m in models \n",
    "            ], axis=0)\n",
    "        )\n",
    "        y.extend(y_batch)\n",
    "    assert len(y) == n_samples\n",
    "    y = np.clip(y, a_min=0, a_max=None)\n",
    "    return pd.DataFrame({\n",
    "        'row_id': df.iloc[:, 0],\n",
    "        'meter_reading': y,\n",
    "    })[['row_id', 'meter_reading']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = load_model_catboost(n_models=7)\n",
    "predict_catboost(test, pipeline_factory(), ms)#.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(pipeline: Pipeline, df: pd.DataFrame, n_jobs: int = -1, **params) -> Tuple[float, float]:\n",
    "    \n",
    "    x = df.drop(columns='meter_reading')\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "\n",
    "    default_params = dict(\n",
    "        n_estimators=10,\n",
    "        max_depth=None,\n",
    "        max_features='auto',\n",
    "        min_samples_leaf=1,\n",
    "    )\n",
    "    merged_params = {**default_params, **params}\n",
    "\n",
    "    pipeline_params = {**merged_params, 'n_jobs': n_jobs}\n",
    "    pipeline_params = add_key_prefix(pipeline_params, 'regressor__')\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "    \n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(merged_params)\n",
    "        scores = cross_validate(\n",
    "            pipeline, x, y,\n",
    "            cv=3,\n",
    "            scoring=rmse_score,\n",
    "            return_train_score=True,\n",
    "            verbose=2,\n",
    "        )\n",
    "        \n",
    "        rmse_val = - np.mean(scores['test_score'])\n",
    "        rmse_train = - np.mean(scores['train_score'])\n",
    "        mlflow.log_metrics(dict(\n",
    "            rmse_val=rmse_val,\n",
    "            rmse_train=rmse_train,\n",
    "        ))\n",
    "        return rmse_val, rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv(\n",
    "    pipeline_factory(),\n",
    "    train,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=64,\n",
    "    min_samples_leaf=0.00001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest One shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(pipeline: Pipeline, df: pd.DataFrame, **params):\n",
    "    \n",
    "    x = df.drop(columns='meter_reading')\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "\n",
    "    default_params = dict(\n",
    "        n_estimators=10,\n",
    "        max_depth=None,\n",
    "        max_features='auto',\n",
    "        min_samples_leaf=1,\n",
    "    )\n",
    "    merged_params = {**default_params, **params}\n",
    "\n",
    "    pipeline_params = {**merged_params, 'n_jobs': -1, 'verbose': 2}\n",
    "    pipeline_params = add_key_prefix(pipeline_params, 'regressor__')\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "\n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(merged_params)\n",
    "\n",
    "        pipeline.fit(x, y)\n",
    "        joblib.dump(pipeline, 'out/pipeline.joblib', compress=1)\n",
    "        \n",
    "        score = rmse(y, pipeline.predict(x))\n",
    "        \n",
    "        mlflow.log_metrics(dict(rmse_train=score))\n",
    "        mlflow.log_artifact('out/pipeline.joblib')\n",
    "        \n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = oneshot(pipeline_factory(), train, n_estimators=64, min_samples_leaf=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(pipeline: Pipeline, df: pd.DataFrame, n_jobs: int = -1, **param_grid):\n",
    "            \n",
    "    x = df.drop(columns='meter_reading')\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "\n",
    "    default_param_grid = dict(\n",
    "        n_estimators=[80],\n",
    "        max_depth=[None],\n",
    "        max_features=['auto'],\n",
    "        min_samples_leaf=[0.00001],\n",
    "    )\n",
    "    merged_param_grid = {**default_param_grid, **param_grid}\n",
    "    pipeline_param_grid = add_key_prefix(merged_param_grid, 'regressor__')\n",
    "    \n",
    "    pipeline.set_params(regressor__n_jobs=n_jobs)\n",
    "    \n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(merged_param_grid)\n",
    "        \n",
    "        regressor = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=pipeline_param_grid,\n",
    "            cv=3,\n",
    "            scoring=rmse_score,\n",
    "            verbose=2,\n",
    "            refit=True,\n",
    "        )\n",
    "\n",
    "        regressor.fit(x, y)\n",
    "        \n",
    "        best_model = regressor.best_estimator_\n",
    "        best_param = add_key_prefix(regressor.best_params_)\n",
    "        best_rmse = - regressor.best_score_\n",
    "        cv_results = df_from_cv_results(regressor.cv_results_)\n",
    "\n",
    "        joblib.dump(best_model, 'out/pipeline.joblib')\n",
    "        cv_results.to_csv('out/cv_results.csv', index=False)\n",
    "        \n",
    "        mlflow.log_params(best_param)\n",
    "        mlflow.log_metrics(dict(\n",
    "            rmse=best_rmse,\n",
    "        ))\n",
    "        mlflow.log_artifact('./out/pipeline.joblib')\n",
    "        mlflow.log_artifact('./out/cv_results.csv')\n",
    "        mlflow.end_run()\n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(\n",
    "    pipeline_factory(),\n",
    "    train,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=[64, 80, 96],\n",
    "    max_features=['auto', 'sqrt'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(run_id: str = None):\n",
    "    if run_id is None:\n",
    "        model_path = 'out/pipeline.joblib'\n",
    "    else:\n",
    "        mlflow_client = mlflow.tracking.MlflowClient()\n",
    "        model_path = mlflow_client.download_artifacts(run_id, 'pipeline.joblib')\n",
    "\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "\n",
    "def predict(df: pd.DataFrame, pipeline: Pipeline) -> pd.DataFrame:\n",
    "    x = df.iloc[:, 1:]\n",
    "    y_log1p = pipeline.predict(x)\n",
    "    y = np.expm1(y_log1p)\n",
    "    return pd.DataFrame({\n",
    "        'row_id': df.iloc[:, 0],\n",
    "        'meter_reading': y,\n",
    "    })[['row_id', 'meter_reading']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = load_model()\n",
    "predict(test, p).to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
