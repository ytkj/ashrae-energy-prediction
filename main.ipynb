{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "CURRENT_EXPERIMENT_NAME = 'feature engineering'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    df_out = df\n",
    "    for key, value in kwargs.items():\n",
    "        if type(value) is list:\n",
    "            df_out = df_out[df_out[key].isin(value)]\n",
    "        else:\n",
    "            df_out = df_out[df_out[key] == value]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def missing_rate(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isnull().sum() / len(df)\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df: pd.DataFrame, verbose: bool = True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / (1024 ** 2)    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem)\n",
    "        )\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "rmse_score = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "def add_key_prefix(d: Dict, prefix = 'best_') -> Dict:\n",
    "    return {prefix + key: value for key, value in d.items()}\n",
    "\n",
    "\n",
    "def df_from_cv_results(d: Dict):\n",
    "    df = pd.DataFrame(d)\n",
    "    score_columns = ['mean_test_score', 'mean_train_score']\n",
    "    param_columns = [c for c in df.columns if c.startswith('param_')]\n",
    "    return pd.concat([\n",
    "        -df.loc[:, score_columns],\n",
    "        df.loc[:, param_columns],\n",
    "    ], axis=1).sort_values(by='mean_test_score')\n",
    "\n",
    "\n",
    "def sample(*args, frac: float = 0.01) -> np.ndarray:\n",
    "    n_rows = args[0].shape[0]\n",
    "    random_index = np.random.choice(n_rows, int(n_rows * frac), replace=False)\n",
    "    gen = (\n",
    "        a[random_index] for a in args\n",
    "    )\n",
    "    if len(args) == 1:\n",
    "        return next(gen)\n",
    "    else:\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nan_weather(w: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # add missing datetime\n",
    "    dt_min, dt_max = w['timestamp'].min(), w['timestamp'].max()\n",
    "    empty_df = pd.DataFrame({'timestamp': pd.date_range(start=dt_min, end=dt_max, freq='H')})\n",
    "    w_out = pd.concat([\n",
    "        ws.merge(\n",
    "            empty_df, on='timestamp', how='outer'\n",
    "        ).sort_values(\n",
    "            by='timestamp'\n",
    "        ).assign(\n",
    "            site_id=site_id\n",
    "        ) for site_id, ws in w.groupby('site_id')\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # large missing rate columns; fill by -999\n",
    "    w_out['cloud_coverage'] = w_out['cloud_coverage'].fillna(-999).astype(np.int16)\n",
    "\n",
    "    # small missing rate columns; fill by same value forward and backward\n",
    "    w_out = pd.concat([\n",
    "        ws.fillna(method='ffill').fillna(method='bfill') for _, ws in w_out.groupby('site_id')\n",
    "    ], ignore_index=True)\n",
    "        \n",
    "    # fill nan by mean over all sites\n",
    "    w_mean = w_out.groupby('timestamp').mean().drop(columns=['site_id']).reset_index()\n",
    "    w_mean = w_out.loc[:, ['site_id', 'timestamp']].merge(w_mean, on='timestamp', how='left')\n",
    "    w_out = w_out.where(~w_out.isnull(), w_mean)\n",
    "    \n",
    "    # float -> uint\n",
    "    w_out['site_id'] = w_out['site_id'].astype(np.uint8)\n",
    "    \n",
    "    return w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_weighted_average(w: pd.DataFrame, hours: int = 5, past: bool = True):\n",
    "    \n",
    "    if past:\n",
    "        fill_method = 'bfill'\n",
    "        sign = 1\n",
    "    else:\n",
    "        fill_method = 'ffill'\n",
    "        sign = -1\n",
    "\n",
    "    def shift_by(wdf: pd.DataFrame, n: int, method: str) -> pd.DataFrame:\n",
    "        return pd.concat([\n",
    "            ws.iloc[:, [2, 4, 8]].shift(n).fillna(method=method) for _, ws in wdf.groupby('site_id')\n",
    "        ], axis=0)\n",
    "        \n",
    "    w_weighted_average = sum(\n",
    "        [shift_by(w, (i+1)*sign, fill_method) * (hours-i) for i in range(hours)]\n",
    "    ) / (np.arange(hours) + 1).sum()\n",
    "    \n",
    "    w_weighted_average.columns = ['{0}_wa{1}'.format(c, sign*hours) for c in w_weighted_average.columns]\n",
    "    \n",
    "    return pd.concat([w, w_weighted_average], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x: pd.DataFrame, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColumnTransformer(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, defs: Dict[str, BaseTransformer]):\n",
    "        self.defs = defs\n",
    "    \n",
    "    def fit(self, x: pd.DataFrame, y: np.ndarray = None):\n",
    "        for col, transformer in self.defs.items():\n",
    "            transformer.fit(x[col], y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        for col, transformer in self.defs.items():\n",
    "            xp[col] = transformer.transform(x[col])\n",
    "        return xp\n",
    "    \n",
    "    def fit_transform(self, x: pd.DataFrame, y: np.ndarray = None) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        for col, transformer in self.defs.items():\n",
    "            if hasattr(transformer, 'fit_transform'):\n",
    "                xp[col] = transformer.fit_transform(x[col], y)\n",
    "            else:\n",
    "                xp[col] = transformer.fit(x[col], y).transform(x[col])\n",
    "        return xp\n",
    "\n",
    "\n",
    "class WrappedLabelEncoder(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, x: pd.Series):\n",
    "        self.encoder = LabelEncoder().fit(x.values)\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return self.encoder.transform(x)\n",
    "    \n",
    "    def fit_transform(self, x: pd.Series, y: np.ndarray = None) -> pd.Series:\n",
    "        return self.transform(x)\n",
    "\n",
    "\n",
    "class BuildingMetaJoiner(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, bm: pd.DataFrame = None):\n",
    "        self.bm = bm\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.bm is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x.merge(\n",
    "                self.bm,\n",
    "                on='building_id',\n",
    "                how='left',\n",
    "            )\n",
    "\n",
    "    \n",
    "class WeatherJoiner(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, w: pd.DataFrame = None):\n",
    "        self.w = w\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.w is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x.merge(\n",
    "                self.w,\n",
    "                on=['site_id', 'timestamp'],\n",
    "                how='left',\n",
    "            )\n",
    "\n",
    "    \n",
    "class ConstantImputer(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, constant: Any):\n",
    "        self.constant = constant\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return x.fillna(self.constant)\n",
    "\n",
    "\n",
    "class MeanImputer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return x.fillna(x.mean())\n",
    "\n",
    "\n",
    "class WindDirectionEncoder(BaseTransformer):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _from_degree(degree: int) -> int:\n",
    "        val = int((degree / 22.5) + 0.5)\n",
    "        arr = [i for i in range(0,16)]\n",
    "        return arr[(val % 16)]\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return x.apply(self._from_degree)\n",
    "\n",
    "\n",
    "class WindSpeedEncoder(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return pd.cut(\n",
    "            x,\n",
    "            bins=[0, 0.3, 1.6, 3.4, 5.5, 8, 10.8, 13.9, 17.2, 20.8, 24.5, 28.5, 33, 1000],\n",
    "            right=False, labels=False,\n",
    "        )\n",
    "\n",
    "\n",
    "class DatetimeFeatureEngineerer(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, col: str = 'timestamp'):\n",
    "        self.col = col\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        ts = x[self.col]\n",
    "        xp['month'] = ts.dt.month.astype(np.int8)\n",
    "        xp['week'] = ts.dt.week.astype(np.int8)\n",
    "        xp['day_of_week'] = ts.dt.weekday.astype(np.int8)\n",
    "        xp['time_period'] = pd.cut(\n",
    "            ts.dt.hour,\n",
    "            bins=[0, 3, 6, 9, 12, 15, 18, 21, 25],\n",
    "            right=False, labels=False,\n",
    "        )\n",
    "        \n",
    "        holidays = [\n",
    "            '2016-01-01', '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04',\n",
    "            '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24', '2016-12-26',\n",
    "            '2017-01-01', '2017-01-16', '2017-02-20', '2017-05-29', '2017-07-04',\n",
    "            '2017-09-04', '2017-10-09', '2017-11-10', '2017-11-23', '2017-12-25',\n",
    "            '2018-01-01', '2018-01-15', '2018-02-19', '2018-05-28', '2018-07-04',\n",
    "            '2018-09-03', '2018-10-08', '2018-11-12', '2018-11-22', '2018-12-25',\n",
    "            '2019-01-01'\n",
    "        ]\n",
    "        xp['is_holiday'] = (ts.dt.date.astype('str').isin(holidays)).astype(np.int8)\n",
    "        return xp\n",
    "\n",
    "class WeatherFeatureEngineerer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        xp = x.copy()\n",
    "        xp['temperature_diff'] = x['air_temperature'] - x['dew_temperature']\n",
    "        xp['temperature_diff_wa5'] = x['air_temperature_wa5'] - x['dew_temperature_wa5']\n",
    "        xp['temperature_diff_wa-5'] = x['air_temperature_wa-5'] - x['dew_temperature_wa-5']\n",
    "        return xp\n",
    "\n",
    "\n",
    "class LogarithmTransformer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.Series) -> pd.Series:\n",
    "        return np.log(x)\n",
    "\n",
    "\n",
    "class TargetEncoder(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, cv: int = 5, smoothing: int = 1):\n",
    "        self.agg = None\n",
    "        self.cv = cv\n",
    "        self.smoothing = 1\n",
    "    \n",
    "    def transform(self, x: pd.Series):        \n",
    "        if self.agg is None:\n",
    "            raise ValueError('you shold fit() before predict()')\n",
    "        encoded = pd.merge(x, self.agg, left_on=x.name, right_index=True, how='left')\n",
    "        encoded = encoded.fillna(encoded.mean())\n",
    "        xp = encoded['y']\n",
    "        xp.name = x.name\n",
    "        return xp\n",
    "    \n",
    "    def fit_transform(self, x: pd.Series, y: np.ndarray = None) -> pd.Series:\n",
    "        df = pd.DataFrame({'x': x, 'y': y})\n",
    "        self.agg = df.groupby('x').mean()\n",
    "        fold = KFold(n_splits=self.cv, shuffle=True)\n",
    "        xp = x.copy()\n",
    "        for idx_train, idx_test in fold.split(x):\n",
    "            df_train = df.loc[idx_train, :]\n",
    "            df_test = df.loc[idx_test, :]\n",
    "            agg_train = df_train.groupby('x').mean()\n",
    "            encoded = pd.merge(df_test, agg_train, left_on='x', right_index=True, how='left', suffixes=('', '_mean'))['y_mean']\n",
    "            encoded = encoded.fillna(encoded.mean())\n",
    "            xp[encoded.index] = encoded\n",
    "        return xp\n",
    "\n",
    "\n",
    "class ColumnDropper(BaseTransformer):\n",
    "    \n",
    "    def __init__(self, cols: List[str]):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame, y = None) -> pd.DataFrame:\n",
    "        return x.drop(columns=self.cols)\n",
    "\n",
    "\n",
    "class ArrayTransformer(BaseTransformer):\n",
    "    \n",
    "    def transform(self, x: pd.DataFrame, y = None) -> np.ndarray:\n",
    "        return x.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv', parse_dates=['timestamp']).pipe(reduce_mem_usage)\n",
    "building_metadata = pd.read_csv('data/building_metadata.csv')\n",
    "weather_train = pd.read_csv('data/weather_train.csv', parse_dates=['timestamp'])\n",
    "\n",
    "test = pd.read_csv('data/test.csv', parse_dates=['timestamp']).pipe(reduce_mem_usage)\n",
    "weather_test = pd.read_csv('data/weather_test.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_factory() -> Pipeline:\n",
    "    return Pipeline(steps=[\n",
    "\n",
    "        # join\n",
    "        ('join_building_meta', BuildingMetaJoiner(building_metadata)),\n",
    "        ('join_weather', WeatherJoiner(\n",
    "            pd.concat([\n",
    "                weather_train,\n",
    "                weather_test\n",
    "            ], axis=0).pipe(\n",
    "                fix_nan_weather\n",
    "            ).pipe(\n",
    "                weather_weighted_average\n",
    "            ).pipe(\n",
    "                weather_weighted_average, past=False\n",
    "            )\n",
    "        )),\n",
    "\n",
    "        # missing value\n",
    "        ('impute_fix_values', ColumnTransformer({\n",
    "            'year_built': ConstantImputer(-999),\n",
    "            'floor_count': ConstantImputer(-999),\n",
    "        })),\n",
    "\n",
    "        # feature engineering\n",
    "        ('label_encode', ColumnTransformer({\n",
    "            'wind_direction': WindDirectionEncoder(),\n",
    "            'wind_speed': WindSpeedEncoder(),\n",
    "            'wind_speed_wa5': WindSpeedEncoder(),\n",
    "            'wind_speed_wa-5': WindSpeedEncoder(),\n",
    "            'primary_use': WrappedLabelEncoder(building_metadata['primary_use']),\n",
    "        })),\n",
    "        ('transform_logarithm', ColumnTransformer({\n",
    "            'square_feet': LogarithmTransformer(),\n",
    "        })),\n",
    "        ('feature_engineering_from_datetime', DatetimeFeatureEngineerer()),\n",
    "        ('target_encode', ColumnTransformer({\n",
    "            'primary_use': TargetEncoder(),\n",
    "            'meter': TargetEncoder(),\n",
    "            'cloud_coverage': TargetEncoder(),\n",
    "            'time_period': TargetEncoder(),\n",
    "            'wind_direction': TargetEncoder(),\n",
    "            'wind_speed': TargetEncoder(),\n",
    "            'wind_speed_wa5': TargetEncoder(),\n",
    "            'wind_speed_wa-5': TargetEncoder(),\n",
    "        })),\n",
    "        ('diff_temperature', WeatherFeatureEngineerer()),\n",
    "\n",
    "        # drop columns\n",
    "        ('drop_columns', ColumnDropper([\n",
    "            'building_id', 'timestamp', 'site_id', 'precip_depth_1_hr',\n",
    "        ])),\n",
    "\n",
    "        # pd.DataFrame -> np.ndarray\n",
    "        ('df_to_array', ArrayTransformer()),\n",
    "\n",
    "        # regressor\n",
    "        ('regressor', RandomForestRegressor()),\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(pipeline: Pipeline, df: pd.DataFrame, n_jobs: int = -1, **params) -> Tuple[float, float]:\n",
    "    \n",
    "    x = df.drop(columns='meter_reading')\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "\n",
    "    default_params = dict(\n",
    "        n_estimators=10,\n",
    "        max_depth=None,\n",
    "        max_features='auto',\n",
    "        min_samples_leaf=1,\n",
    "    )\n",
    "    merged_params = {**default_params, **params}\n",
    "\n",
    "    pipeline_params = {**merged_params, 'n_jobs': n_jobs}\n",
    "    pipeline_params = add_key_prefix(pipeline_params, 'regressor__')\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "    \n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(merged_params)\n",
    "        scores = cross_validate(\n",
    "            pipeline, x, y,\n",
    "            cv=3,\n",
    "            scoring=rmse_score,\n",
    "            return_train_score=True,\n",
    "            verbose=2,\n",
    "        )\n",
    "        \n",
    "        rmse_val = - np.mean(scores['test_score'])\n",
    "        rmse_train = - np.mean(scores['train_score'])\n",
    "        mlflow.log_metrics(dict(\n",
    "            rmse_val=rmse_val,\n",
    "            rmse_train=rmse_train,\n",
    "        ))\n",
    "        return rmse_val, rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=21.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 23.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    }
   ],
   "source": [
    "cv(\n",
    "    pipeline_factory(),\n",
    "    train,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=64,\n",
    "    min_samples_leaf=0.00001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(pipeline: Pipeline, df: pd.DataFrame, **params):\n",
    "    \n",
    "    x = df.drop(columns='meter_reading')\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "\n",
    "    default_params = dict(\n",
    "        n_estimators=10,\n",
    "        max_depth=None,\n",
    "        max_features='auto',\n",
    "        min_samples_leaf=1,\n",
    "    )\n",
    "    merged_params = {**default_params, **params}\n",
    "\n",
    "    pipeline_params = {**merged_params, 'n_jobs': -1, 'verbose': 2}\n",
    "    pipeline_params = add_key_prefix(pipeline_params, 'regressor__')\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "\n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(merged_params)\n",
    "\n",
    "        pipeline.fit(x, y)\n",
    "        joblib.dump(pipeline, 'out/pipeline.sav', compress=1)\n",
    "        \n",
    "        score = rmse(y, pipeline.predict(x))\n",
    "        \n",
    "        mlflow.log_metrics(dict(rmse_train=score))\n",
    "        mlflow.log_artifact('out/pipeline.sav')\n",
    "        \n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 out of  64 | elapsed: 35.4min remaining: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed: 36.0min finished\n",
      "[Parallel(n_jobs=32)]: Done  34 out of  64 | elapsed:   37.1s remaining:   32.7s\n",
      "[Parallel(n_jobs=32)]: Done  64 out of  64 | elapsed:   43.9s finished\n"
     ]
    }
   ],
   "source": [
    "p = oneshot(pipeline_factory(), train, n_estimators=64, min_samples_leaf=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(pipeline: Pipeline, df: pd.DataFrame, n_jobs: int = -1, **param_grid):\n",
    "            \n",
    "    x = df.drop(columns='meter_reading')\n",
    "    y = np.log1p(df['meter_reading'].values)\n",
    "\n",
    "    default_param_grid = dict(\n",
    "        n_estimators=[80],\n",
    "        max_depth=[None],\n",
    "        max_features=['auto'],\n",
    "        min_samples_leaf=[0.00003],\n",
    "    )\n",
    "    merged_param_grid = {**default_param_grid, **param_grid}\n",
    "    pipeline_param_grid = add_key_prefix(merged_param_grid, 'regressor__')\n",
    "    \n",
    "    pipeline.set_params(regressor__n_jobs=n_jobs)\n",
    "    \n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(merged_param_grid)\n",
    "        \n",
    "        regressor = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=pipeline_param_grid,\n",
    "            cv=3,\n",
    "            scoring=rmse_score,\n",
    "            verbose=2,\n",
    "            refit=True,\n",
    "        )\n",
    "\n",
    "        regressor.fit(x, y)\n",
    "        \n",
    "        best_model = regressor.best_estimator_\n",
    "        best_param = add_key_prefix(regressor.best_params_)\n",
    "        best_rmse = - regressor.best_score_\n",
    "        cv_results = df_from_cv_results(regressor.cv_results_)\n",
    "\n",
    "        joblib.dump(best_model, 'out/model.sav')\n",
    "        cv_results.to_csv('out/cv_results.csv', index=False)\n",
    "        \n",
    "        mlflow.log_params(best_param)\n",
    "        mlflow.log_metrics(dict(\n",
    "            rmse=best_rmse,\n",
    "        ))\n",
    "        mlflow.log_artifact('./out/model.sav')\n",
    "        mlflow.log_artifact('./out/cv_results.csv')\n",
    "        mlflow.end_run()\n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(\n",
    "    pipeline,\n",
    "    x=train.drop(columns='meter_reading'),\n",
    "    y=np.log1p(train['meter_reading'].values),\n",
    "    n_jobs=-1,\n",
    "    n_estimators=[64, 80, 96],\n",
    "    max_depth=[12, 13, 14, 15],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(run_id: str = None):\n",
    "    if run_id is None:\n",
    "        model_path = 'out/model.joblib'\n",
    "    else:\n",
    "        mlflow_client = mlflow.tracking.MlflowClient()\n",
    "        model_path = mlflow_client.download_artifacts(run_id, 'model.joblib')\n",
    "\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "\n",
    "def predict(df: pd.DataFrame, pipeline: Pipeline) -> pd.DataFrame:\n",
    "    x = df.iloc[:, 1:]\n",
    "    y_log1p = pipeline.predict(x)\n",
    "    y = np.expm1(y_log1p)\n",
    "    return pd.DataFrame({\n",
    "        'row_id': df.iloc[:, 0],\n",
    "        'meter_reading': y,\n",
    "    })[['row_id', 'meter_reading']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = load_model('05ab3ff87a094178af6c369add6ab39c')\n",
    "predict(test, p).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 1.05G/1.05G [00:27<00:00, 41.0MB/s]\n",
      "Successfully submitted to ASHRAE - Great Energy Predictor III"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"weighted average\""
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
