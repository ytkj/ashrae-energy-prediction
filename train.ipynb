{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import joblib\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_EXPERIMENT_NAME = 'label encoding + basic feature engineering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred) -> float:\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "rmse_score = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_key_prefix(d: Dict, prefix = 'best_') -> Dict:\n",
    "    return {prefix + key: value for key, value in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_cv_results(d: Dict):\n",
    "    df = pd.DataFrame(d)\n",
    "    score_columns = ['mean_test_score', 'mean_train_score']\n",
    "    param_columns = [c for c in df.columns if c.startswith('param_')]\n",
    "    return pd.concat([\n",
    "        -df.loc[:, score_columns],\n",
    "        df.loc[:, param_columns],\n",
    "    ], axis=1).sort_values(by='mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(a: np.ndarray, frac: float = 0.01) -> np.ndarray:\n",
    "    return a[np.random.choice(a.shape[0], int(a.shape[0] * frac), replace=False), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(ds: np.ndarray, n_jobs: int = -1):\n",
    "    \n",
    "    y = np.log1p(ds[:, 0])\n",
    "    x = ds[:, 1:]\n",
    "        \n",
    "    param_grid = dict(\n",
    "        n_estimators=[100],\n",
    "        max_depth=[None],\n",
    "        max_features=['auto'],\n",
    "        min_samples_leaf=[0.0001, 0.0003, 0.0006],\n",
    "    )\n",
    "        \n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        mlflow.log_params(param_grid)\n",
    "        \n",
    "        regressor = GridSearchCV(\n",
    "            RandomForestRegressor(n_jobs=n_jobs),\n",
    "            param_grid=param_grid,\n",
    "            cv=3,\n",
    "            scoring=rmse_score,\n",
    "            verbose=2,\n",
    "            refit=True,\n",
    "        )\n",
    "\n",
    "        regressor.fit(x, y)\n",
    "        \n",
    "        best_model = regressor.best_estimator_\n",
    "        best_param = add_key_prefix(regressor.best_params_)\n",
    "        best_rmse = - regressor.best_score_\n",
    "        cv_results = df_from_cv_results(regressor.cv_results_)\n",
    "\n",
    "        joblib.dump(best_model, 'out/model.sav')\n",
    "        cv_results.to_csv('out/cv_results.csv', index=False)\n",
    "        \n",
    "        mlflow.log_params(best_param)\n",
    "        mlflow.log_metrics(dict(\n",
    "            rmse=best_rmse,\n",
    "        ))\n",
    "        mlflow.log_artifact('./out/model.sav')\n",
    "        mlflow.log_artifact('./out/cv_results.csv')\n",
    "        mlflow.end_run()\n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(ds: np.ndarray, n_jobs: int = -1, **params):\n",
    "    default_params = dict(\n",
    "        n_estimators=10,\n",
    "        max_depth=None,\n",
    "        max_features='auto',\n",
    "        min_samples_leaf=1,\n",
    "    )\n",
    "    merged_params = {**default_params, **params}\n",
    "    model = RandomForestRegressor(**merged_params, n_jobs=n_jobs)\n",
    "\n",
    "    mlflow.set_experiment(CURRENT_EXPERIMENT_NAME)\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(merged_params)\n",
    "        model.fit(ds[:, 1:], np.log1p(ds[:, 0]))\n",
    "        joblib.dump(model, 'out/model.sav')\n",
    "        mlflow.log_artifact('./out/model.sav')\n",
    "        mlflow.log_metrics(dict(\n",
    "            rmse=-999,\n",
    "        ))\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = np.load('e01_label_encoding_and_basic_feature_engineering/dataset_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search(sample(dataset_train, frac=0.2), n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
